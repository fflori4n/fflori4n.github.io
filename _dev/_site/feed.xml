<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-09-01T23:16:27+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">fFlori4n’s maker blog</title><subtitle>This is my sort of blog related to personal projects. The most consistent thing for my projects is that they are all poorly documented - so I am trying to change that now. I already paid good money for this domain so might as well have a blog/website on it.
</subtitle><author><name>fFlori4n</name></author><entry><title type="html">nod3-comfirm - Common firmware for ESP32 based devices</title><link href="http://localhost:4000/2025/08/03/nod3-comfirm.html" rel="alternate" type="text/html" title="nod3-comfirm - Common firmware for ESP32 based devices" /><published>2025-08-03T00:00:00+02:00</published><updated>2025-08-03T00:00:00+02:00</updated><id>http://localhost:4000/2025/08/03/nod3-comfirm</id><content type="html" xml:base="http://localhost:4000/2025/08/03/nod3-comfirm.html"><![CDATA[<p><strong>nod3-comfirm</strong> - is an ESP_IDF based firmware intended to be used as a template/ starting point for future IoT projects. It is primarily written for ESP C3.
Using ESP_IDF and modern(ish) C++ ( <em>to avoid the usual cowboy enthics of C projects ~ though it is less efficient</em> ), nod3-comfirm aims to be used as a basic firmware for smart sensor devices. Firmware has basic functions that are needed for every IoT thing, like:</p>
<ul>
  <li>Network management (WLAN)</li>
  <li>IO Control</li>
  <li>ADC continous/ DMA and other ADC functions</li>
  <li>Time keeping, NTP and RTC functions</li>
  <li>Websocket communication with Homeassistant custom component</li>
  <li>libraries for common sensors using I2C</li>
</ul>

<p><strong>nod3</strong> can be modified to taylor it to the specific project, add support for new sensors, or other application specific functions.</p>

<p><img src="http://192.168.0.199:3000/d/detfu7jht6r5sb/hive-scale-test?orgId=1&amp;viewPanel=1" /></p>

<p>404: Not Found</p>]]></content><author><name>fFlori4n</name></author><summary type="html"><![CDATA[nod3-comfirm - is an ESP_IDF based firmware intended to be used as a template/ starting point for future IoT projects. It is primarily written for ESP C3. Using ESP_IDF and modern(ish) C++ ( to avoid the usual cowboy enthics of C projects ~ though it is less efficient ), nod3-comfirm aims to be used as a basic firmware for smart sensor devices. Firmware has basic functions that are needed for every IoT thing, like: Network management (WLAN) IO Control ADC continous/ DMA and other ADC functions Time keeping, NTP and RTC functions Websocket communication with Homeassistant custom component libraries for common sensors using I2C]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/image_1.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/image_1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Prosthetic hand - 3d printed partial prosthetic</title><link href="http://localhost:4000/2025/01/20/prosthetic-hand.html" rel="alternate" type="text/html" title="Prosthetic hand - 3d printed partial prosthetic" /><published>2025-01-20T00:00:00+01:00</published><updated>2025-01-20T00:00:00+01:00</updated><id>http://localhost:4000/2025/01/20/prosthetic-hand</id><content type="html" xml:base="http://localhost:4000/2025/01/20/prosthetic-hand.html"><![CDATA[<p>This is an RnD project, working with a team of three other engineers. Our goal is to develop a prototype for a partial hand prosthetic that can be 3D printed and built with basic hobbyist level parts. This design unlike other prosthetics, is a partial prosthetic, meaning it is worn on the wrist, and tries to replicate the function of the four ‘axial’ fingers - the main mechanical constraint is space as all mechanisms need to clear the hand and the thumb. A custom PCB with an STM chip is responsible for controlling the motors, with force and positioning feedback.</p>

<p>The project details can be read in the git repo: <a href="http://github.com/AleksaHeler/OpenHand"> http://github.com/AleksaHeler/OpenHand </a> , there is a lot of stuff, and I mean <strong>a lot</strong>, definitely worth the read.</p>

<p>there are so many aspects of this project, I will only write a brief about lessons learned related to two topics:</p>
<ul>
  <li>mechanical design</li>
  <li>motor feedback</li>
</ul>

<h3 id="some-thoughts-about-the-mechanism">Some thoughts about the mechanism</h3>

<p>There is a lot of ‘robotic hand’ projects on the internet, it is commonly developed for humanoid robots, prosthetics or student projects. From the mechanics side, there are two common approaches:</p>
<ul>
  <li>Cable driven fingers - this allows the finger to have better articulation, the motors are located at a distance. The downside of this approach is durability and size/ weight, grip strength is fairly weak, more parts, finicky cables and a lots of moving components.</li>
  <li>Rigid linkage - this is our chosen approach, the finger is actuated directly by a single motor - this means that there is only a single degree of freedom per finger, the fingertip (dist. phalanx) and the middle part of the finger (middle phalanx) move on a predetermined trajectory that emulates the natural movement a finger would take when gripping an object.</li>
</ul>

<p>After cost/benefit analysis, some shortcuts were cut:</p>
<ul>
  <li>a finger only does ‘grip’ (<em>look up grasping patterns, there is a lot of them, grip is only one of these possible patterns</em>) - it is not ideal, but it will keep the mechanics side somewhat simple.</li>
  <li>pinky finger is not implemented by prosthesis - it is useful for holding e.g. a cup, but the same can be done with only 3 fingers if using ring finger instead of pinky - this is due to the space and weight constraints.</li>
</ul>

<h4 id="design-1">Design 1:</h4>

<p>Using a PWM servo for moving the finger - pretty bad idea, but it is easy to test, we were only interested if the linkage actually works, as it was designed mostly using trial and error positioning of a constrained skeleton geometry in CAD. It worked, but there were some things to improve e.g. the mechanism can not be extended fully, as it crosses a singularity point and locks up. This can be seen when the dist phalanx locks up if pushed backwards (to open more) in the fully extended position - Of course that’s an issue, the real human finger does not like that either…</p>

<p><img src="https://github.com/fflori4n/fflori4n/raw/main/assets/pics/hand_V07.png" alt="" style="width: 50%; height:auto;" /></p>

<h4 id="design-2">Design 2:</h4>
<p>This PWM servo solution has another problem - it just wont fit. You can’t fit 3 of these mechanisms next to each other.
So the next step was replacing this servo, with a linear DC motor - this made it work much better, as there is one less joint in the linkage, it is stronger (but a little bit back drivable and flexible due to the mechanism tolerances and plastic parts).</p>

<p><img src="https://github.com/fflori4n/fflori4n/raw/main/assets/pics/fingerV2.png" alt="" style="width: 50%; height:auto;" /></p>

<h4 id="design-3">Design 3:</h4>
<p>After this the mechanism could be optimized some more to be as lightweight as possible but still strong. The finger has a tendency to bend to one side when force is applied in the plain of the palm (<em>in other designs this is usually mitigated by supporting the joint on both sides, but no space for it here</em>) - this is not a big deal but it also shows up when gripping objects that have an angle to them, which is bad. The finger has to be somewhat bigger to not snap - the proxy phalanx can exsert a strong grip, the mid phalanx a little bit weaker, the dist phalanx is pretty weak, but it is somewhat flexible and can bend onto the object when gripping, it is springy, very interesting effect.</p>

<h3 id="force-feedback-and-positioning">Force feedback and positioning</h3>

<h4 id="current-limit---grip-limit">Current limit - grip limit</h4>
<p>Design one and two, use only force feedback. This is done by regulating the current to the DC motors. We found that when the current is not limited, the consumption is about 500mA per finger - this is a very strong grip. When limit is applied the hand can be used to grip ‘soft’ objects like an empty plastic bottle without crushing it - this works ok.</p>

<p>For design 3, we wanted positioning. This is a complicated problem, as it has to be space constrained, and also retrofitted onto the existing parts.</p>

<h4 id="analog-hall-sensors">Analog hall sensor(s)</h4>
<p>We experimented with analog hall sensors. A hall sensor E49 is mounted to the metacarpal, the magnet is mounted on proxy phalanx - one hall sensor either can’t give enough resoulution, or can’t measure the absolute position over the whole trajectory of the fingertip. This would work for rough  positioning or for position feedback only when finger is almost closed to grip objects.
To work around this, we tried two hall sensors offset from each other, to ger two curves as:</p>

<p><img src="https://github.com/fflori4n/fflori4n/raw/main/assets/pics/hall_feedback_pattern.png" alt="" style="width: 80%; height:auto;" /></p>

<p>This looks promising, because the absolute position can be determined by combining the two analog voltages to get a lookup table for position (<em>or angle of metacarpal and proxy phalanx which determines finger tip position</em>), and also knowing <strong>if the finger is currently moving to open or to close, or at least the previous position</strong>
Downsides are: 2 hall * 3 fingers = 6 analog voltages routed to the MCU; 6 channels used for ADC; enviroment can be noisy;</p>

<p>Another idea could be using a hall sensor like the AS5600 IC that uses a radially polarized magnet to sense absolute 12bit angle and generate PWM or I2C output. This is a fairly precise sensor - <em>had good experience using AS5600 for wind angle detection before</em> - but placing the magnet onto the joint is not possible in this design.</p>

<h4 id="using-an-encoder">Using an encoder</h4>
<p>What if we could use the motor itself for positioning? The linear actuator when broken open is just a DC motor with a lead screw. It has wipers on a pcb with mosfets to serve as endstops and stop the motor at the end of travel. Bigger motors can have closed loop control, with feedback from an optical or hall encoder - this principle is used to get servo motors (even with high torque BLDC motors) for robotics applications.
We did not find any off the shelf linear actuator with positioning capability in this small size - so we made one. The linear actuator contains a regular DC motor, the exact same DC motor with hall encoder attached can be bought online. We replaced the motor assembly in the linear actuator to use the motor with hall effect encoder, and printed a new housing for it, it took some careful disassembly and HSS shaft filing but it worked out great.
Our assumption is that these motors could be custom made easily and cheaply, they just don’t currently exist - if we ever need bigger quantities for a batch of prosthetics.</p>

<p><img src="https://github.com/fflori4n/fflori4n/raw/main/assets/pics/linear_motor.png" alt="" style="width: 100%; height:auto;" /></p>

<p>The modifications to the motor (and the internals of these linear motors) are shared on grabcad:
<a href="https://grabcad.com/library/geared-motor-with-encoder-for-linear-actuator-1"> https://grabcad.com/library/geared-motor-with-encoder-for-linear-actuator-1 </a></p>]]></content><author><name>fFlori4n</name></author><summary type="html"><![CDATA[This is an RnD project, working with a team of three other engineers. Our goal is to develop a prototype for a partial hand prosthetic that can be 3D printed and built with basic hobbyist level parts. This design unlike other prosthetics, is a partial prosthetic, meaning it is worn on the wrist, and tries to replicate the function of the four ‘axial’ fingers - the main mechanical constraint is space as all mechanisms need to clear the hand and the thumb. A custom PCB with an STM chip is responsible for controlling the motors, with force and positioning feedback.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/post-prosthetic-hand/prostetic-hand-v08.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/post-prosthetic-hand/prostetic-hand-v08.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Plant detection for agricultural drone using OpenCV computer vision</title><link href="http://localhost:4000/2023/02/10/plant-mass-detection.html" rel="alternate" type="text/html" title="Plant detection for agricultural drone using OpenCV computer vision" /><published>2023-02-10T00:00:00+01:00</published><updated>2023-02-10T00:00:00+01:00</updated><id>http://localhost:4000/2023/02/10/plant-mass-detection</id><content type="html" xml:base="http://localhost:4000/2023/02/10/plant-mass-detection.html"><![CDATA[<p>R&amp;D project in collaboration with Drontech (<a href="https://www.facebook.com/dronteh">Drontech Facebook page</a>) to develop a proof of concept for a device that uses computer vision to estimate plant mass beneath a drone.
A GPIO pin is set to HIGH when plant mass exceeds a defined threshold, the signal is intended to control sprayer valves, turning off pesticide flow when the area under the flight path lacks viable cultivation or the plants have dried out, avoiding unnecessary pesticide use on unproductive areas.</p>

<p><img src="https://github.com/fflori4n/fflori4n/raw/main/assets/pics/droneWGreenLed.png" alt="drone with cam device under it" style="width: 42.5%; height:auto;" /></p>

<p><em>This is a picture of the “flight article” rpi, it can be seen fixed at the bottom of the drone, the camera is looking forward-down. yes, rpi is getting absolutely blasted by ground obstacle radar which is probably not good for the pi or the radar</em></p>

<h4 id="timeline">Timeline</h4>

<p>Developement was done in two stages:</p>
<ul>
  <li>first a CV algorith was created in python OpenCV based on recorded video footage. The clasification of plants is based only on color and noise/texture. - using YOLO V4 for weed classification was considered but abbadoned due to the limited hardware capabilities, and because it was only a proof of concept.</li>
  <li>after that the computer vision algorithm was rewritten in OpenCV C++ to allow real time processing.</li>
</ul>

<h4 id="cv-algorithm">CV algorithm</h4>

<p><img src="https://github.com/fflori4n/fflori4n/raw/main/assets/pics/drone_plant_detect.gif" alt="gif plant detect" style="width: 100%; height:auto;" /></p>

<p>This gif shows the testing of the algorithm. I cycle through multiple sources of footage previously collected by the drone, and looking at the highlighted plants - then tune the parameters or the algorithm and repeat.
This part of the developement uses python, because it is flexible and easy to change - but it is in no way real time, or running on actual hardware, that comes later.</p>

<p>The algorithm works based on pixel colors, and texture roughness. Basically this is the simplest way to detect plants, but simple is good because the algorithm will be run on a RPI4. This is not a great solution  because it depends on lighting conditions to a degree - harshe sunlight/ shadows can mess up the detector and blind the camera.</p>

<p>Segmentation based on color sounds simple, and it is, but it takes a lot of fiddleing with the parameters to get it to work.</p>

<p>Actually the detection uses a few steps:</p>
<ul>
  <li>drop the resolution, resize with closest interpolation and also reduce the color depth - because of processing limitations.</li>
  <li>transform it using 4D matrixes (image transform matrixes? I forget what they’re called), so the image represents a top down view of the area in front of the camera - this is why some videos on the gif are funnel shaped</li>
  <li>convert to CSV</li>
  <li>create “light intensity/ saturation too low/ too high” mask - obviously if camera sees only black or white thats probably not plants. If the image is washed out, saturation is low - we can not tell the difference between colors, so ignore that aswell.</li>
  <li>create “brown mask” - this will detect dirt, later will be used as a negative mask</li>
  <li>create “green mask” - the actual plants.</li>
  <li>combine all masks to get only the green positive pixels</li>
  <li>filter out small groupings of green-positive pixels (this is done by resizing to small, then back to large mat using CV function, found this is faster then convulution/kernel based filters)</li>
</ul>

<h4 id="the-flight-hardware">The “flight hardware”</h4>

<p><img src="https://github.com/fflori4n/fflori4n/raw/main/assets/pics/rpiDroneCam.jpg" alt="gif plant detect" style="width: 66%; height:auto;" />
<img src="https://github.com/fflori4n/fflori4n/raw/main/assets/pics/rpiFlying.png" alt="gif plant detect" style="width: 66%; height:auto;" /></p>

<p>The algorithm runs on a Raspberry Pi 4 using OpenCV C++ for real-time frame processing. Due to limited processing power the frame rate is relatively low (around 15 FPS).
For testing purposes, a green LED was connected to the GPIO output - <em>the one green dot seen on the picture with the drone</em> - (the status of the output is sent also via WiFi telemetry) - and the drone flown over roads, bare land and different kinds of crops and roadside weeds.</p>

<p>Overall performance was okay, but the functions were very limited. This system can not tell the difference between weeds and regular plants.</p>

<h4 id="in-conclusion">In conclusion</h4>

<p>A better system could use some kind of tenserflow + YOLO + (external hardware CV accelerator) to perform plant classification, and get the actual area and exact location of the weeds.
Actually, this guy on youtube build something very similar to that idea: 
<a href="https://www.youtube.com/watch?v=Fflbc_y2IGQ"> https://www.youtube.com/@nathanbuildsdiy </a></p>
<ul>
  <li>it is still not realtime, but still this is next level stuff for a hobby project. Worth the watch you will be very impressed. ** Also the weed burning magnifying glass robot is awesome **</li>
</ul>

<p>Also I was very dissapointed with the lack of developer access to the drone’s software - it exists on paper, but very hard to get, and expensive - ideally, the drone would provide an API to get the camera feed, GPS location, speed or a way to turn on/off the pump - and the processing could be done remotely on some beefy laptop, right now the drone needs to carry additional weight and it needs to be modified to allow switching of the sprayers - this could be entirely a software feature if the manufacturer opened a developer API to the drone. This drone was from one of the most famous chinese drone makers (<em>if you know you know</em>), but sadly I expect other manufacturers to not be any better in this aspect.</p>

<p>So ultimately weed detection and selective spraying will probably be implemented by the manufacturer of the drone at some point - but still this was a fun project to work on.</p>]]></content><author><name>fFlori4n</name></author><summary type="html"><![CDATA[R&amp;D project in collaboration with Drontech (Drontech Facebook page) to develop a proof of concept for a device that uses computer vision to estimate plant mass beneath a drone. A GPIO pin is set to HIGH when plant mass exceeds a defined threshold, the signal is intended to control sprayer valves, turning off pesticide flow when the area under the flight path lacks viable cultivation or the plants have dried out, avoiding unnecessary pesticide use on unproductive areas.]]></summary></entry></feed>